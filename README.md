【線形回帰モデル】
回帰問題を解く手法で教師あり学習の一つ。
説明変数または特徴量を入力し、ｍ次元のパラメータを持つ目的変数を出力するモデルである。
目的変数は入力とパラメータの内積であり線形結合と呼ぶ。
パラメータは特徴量が予測に与える影響をどのくらいにするかを決定する尺度である。
このパラメータは学習データの平均二乗誤差を最小とする最小二乗法で探索していく。

【非線形回帰モデル】
非線形構造を内在する現象に対して基底関数を用いて推定していくモデル。
未知のパラメータを求めていく際には、学習データに対して小さな誤差を得られていない未学習な状態と
小さな誤差ではあるがテスト誤差との差が大きい過学習な状態とがある。
適切な表現力を持つモデルにするために、故意的に正則化（L1・L2ノルム）や不要な変数を削除するなどのペナルティを課す。
汎化性能の高いモデルを選択するために、ホールドアウト法や交差検証、グリッドリサーチといった手法を用いて評価していく。

【ロジスティック回帰モデル】
ある入力に対してクラス分類を行うモデル。予測もするモデルではないことに留意する。
シグモイド関数を用いて０から１の確率的な数値で決定的な分類をする。
ベルヌーイ分布によりパラメータの最尤推定をしていく。
その際には勾配降下法も用いてパラメータの更新を行っていく。
評価手法としては、正解率がよく使われ、どんな予測をしたいかによって再現率、適合率なども利用する。

【主成分分析】
多次元なデータに対して情報の特徴を残しつつ変量を減らしたいときに使うモデル。
情報の分散が最大となる射影軸を探索し特徴を最適化していく。
ラグランジュ係数を用いて制約条件の付いた最適化問題を探索していく。
元のデータの分散共分散行列の固有値と固有ベクトルがその解となる。
第K主成分の情報量を示す寄与率や第１－K主成分までの圧縮した際の情報損失量を示す
累積寄与率によってどんな特徴をもった成分なのかを把握していく。

【K近傍法】
分類する点からK個の近傍している点を選び、最も多く所属するクラスに分類するモデル。
Kが変化すると結果も変わり、Kを大きくすると決定境界は滑らかになる。
【K-means法】
